> 渡一

文件需要hash
hash算法可用md5实现
md5的库有spark-md5

file调用slice即可得到blob对象
File 和 Blob对象只记录文件的信息，没有资源的内容
要得到内容要使用FileReader去读取
### 实现
```typescript
fileInput.onchange = async function () {
  const file = this.files[0]
  // 使用
  const chunks = createChunks(file, 10 * 1024 * 1024)
  console.log("chunks: ", chunks)
  const fileHash = await hash(chunks) // 得到file的hash值
  console.log("fileHash: ", fileHash)
}
function createChunks(file, chunkSize) {
  const result = []
  for (let i = 0; i < file.size; i += chunkSize) {
    result.push(file.slice(i, i + chunkSize))
  }
  return result
}
// 增量算法
// 算hash 因为一个文件太大，cpu内存吃不消，通过调用spark.append一片片去算
function hash(chunks) {
  return new Promise((resolve, reject) => {
    const spark = new SparkMD5() // spark-md5库
    function _read(i) {
      if (i >= chunks.length) {
        resolve(spark.end()) // 返回hash值
        return
      }
      const blob = chunks[i]
      const reader = new FileReader()
      reader.onload = e => {
        const bytes = e.target.result
        spark.append(bytes)
        _read(i + 1)
      }
      reader.readAsArrayBuffer(blob)
    }
    _read(0)
  })
}
```
### web-worker
由于耗时，可以放到web worker里进行处理
```typescript
function getHashByWorker(chunkList) {
  return new Promise(resolve => {
    const worker = new Worker("./hash.js")
    worker.postMessage({ chunkList })
    worker.onmessage = e => {
      const { hash } = e.data
      if (hash) {
        resolve(hash)
      }
    }
  })
}

// hash.js
self.importScripts("./spark-md5.min.js")
self.onmessage = e => {
    const { chunkList } = e.data
    hash(chunkList).then(res => {
        self.postMessage({
            hash: res,
        })
    })
}

function hash(chunks) {
    return new Promise((resolve, reject) => {
        const spark = new SparkMD5() // spark-md5库
        function _read(i) {
            if (i >= chunks.length) {
                resolve(spark.end()) // 返回hash值
                return
            }
            const blob = chunks[i]
            const reader = new FileReader()
            reader.onload = e => {
                const bytes = e.target.result
                spark.append(bytes)
                _read(i + 1)
            }
            reader.readAsArrayBuffer(blob)
        }
        _read(0)
    })
}

```
目录结构![image.png](https://raw.githubusercontent.com/xxxsjan/pic-bed/main/202305151303941.png)
